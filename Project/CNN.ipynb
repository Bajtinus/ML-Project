{
 "cells": [
  {
   "cell_type": "code",
   "id": "848cea1e7d1017b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:36:07.150477Z",
     "start_time": "2024-12-15T07:36:07.147110Z"
    }
   },
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "ROOT = pathlib.Path(\"D:\\\\ML_Dataset\")\n",
    "PATH = pathlib.Path(ROOT / \"bite_size\" / \"training\")\n",
    "TEST_PATH = pathlib.Path(\"D:\\\\ML_Dataset\\\\bite_size\\\\test\")\n",
    "SAMPLE_SIZE = 50"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:36:07.165281Z",
     "start_time": "2024-12-15T07:36:07.162397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Load image with target size matching training input\n",
    "    image = load_img(image_path, target_size=(150, 150))\n",
    "\n",
    "    # Convert image to array\n",
    "    image_array = img_to_array(image)\n",
    "\n",
    "    # Expand dimensions to fit model input shape (batch size, height, width, channels)\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "    # Normalize the image array as done during model training\n",
    "    image_array /= 255.0\n",
    "\n",
    "    return image_array"
   ],
   "id": "228c7fb69e8b5c29",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:36:07.173875Z",
     "start_time": "2024-12-15T07:36:07.171109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_image_class(image_path):\n",
    "    processed_image = preprocess_image(image_path)\n",
    "\n",
    "    # Predict the class probabilities for the image\n",
    "    prediction = model.predict(processed_image)\n",
    "\n",
    "    # Get the class with the highest probability\n",
    "    predicted_class = prediction.argmax(axis=-1)  # Retrieves the index of the highest value in the array\n",
    "\n",
    "    return predicted_class"
   ],
   "id": "890f0e55cb71b550",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:36:07.180713Z",
     "start_time": "2024-12-15T07:36:07.177884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, path in enumerate(pathlib.Path(PATH).iterdir()):\n",
    "    if i == SAMPLE_SIZE:\n",
    "        break\n",
    "    print(path.name)"
   ],
   "id": "e4c39a5f421165af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copepods\n",
      "diatoms\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:04.477467Z",
     "start_time": "2024-12-15T07:38:04.452196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(PATH,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    batch_size=SAMPLE_SIZE,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(PATH,\n",
    "                                                              target_size=(150, 150),\n",
    "                                                              batch_size=SAMPLE_SIZE,\n",
    "                                                              class_mode='binary')"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 images belonging to 2 classes.\n",
      "Found 59 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:05.589726Z",
     "start_time": "2024-12-15T07:38:05.554454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n"
   ],
   "id": "2842665144dc09b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:07.577644Z",
     "start_time": "2024-12-15T07:38:07.573219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ],
   "id": "7de6827ec7b40886",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:08.171979Z",
     "start_time": "2024-12-15T07:38:08.169244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ],
   "id": "2931d316b670a4c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16725138407659446806\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:09.770656Z",
     "start_time": "2024-12-15T07:38:09.768089Z"
    }
   },
   "cell_type": "code",
   "source": "tf.config.list_physical_devices('GPU')",
   "id": "5fa106efa955a678",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:10.122377Z",
     "start_time": "2024-12-15T07:38:10.119812Z"
    }
   },
   "cell_type": "code",
   "source": "tf.test.is_built_with_cuda()",
   "id": "38d7b2857c951b2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:16.757198Z",
     "start_time": "2024-12-15T07:38:11.316055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.fit(train_generator,\n",
    "              steps_per_epoch=200,\n",
    "              epochs=10,\n",
    "              validation_data=validation_generator,\n",
    "              validation_steps=len(validation_generator))"
   ],
   "id": "8bb87955dc127184",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.6948 - loss: 0.8600 - val_accuracy: 0.7966 - val_loss: 0.5030\n",
      "Epoch 2/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6948 - loss: 0.6006 - val_accuracy: 0.7288 - val_loss: 0.3802\n",
      "Epoch 3/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6953 - loss: 0.4882 - val_accuracy: 0.6949 - val_loss: 0.4531\n",
      "Epoch 4/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 774us/step - accuracy: 0.6951 - loss: 0.4318 - val_accuracy: 0.9153 - val_loss: 0.2729\n",
      "Epoch 5/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8803 - loss: 0.3578 - val_accuracy: 0.9831 - val_loss: 0.1873\n",
      "Epoch 6/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8814 - loss: 0.3158 - val_accuracy: 0.9492 - val_loss: 0.1660\n",
      "Epoch 7/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8814 - loss: 0.3073 - val_accuracy: 0.9831 - val_loss: 0.1206\n",
      "Epoch 8/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 822us/step - accuracy: 0.9830 - loss: 0.1413 - val_accuracy: 0.9831 - val_loss: 0.0732\n",
      "Epoch 9/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9325 - loss: 0.2639 - val_accuracy: 0.9831 - val_loss: 0.0449\n",
      "Epoch 10/10\n",
      "\u001B[1m200/200\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9494 - loss: 0.1103 - val_accuracy: 0.9492 - val_loss: 0.1448\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:18.206664Z",
     "start_time": "2024-12-15T07:38:18.061193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loss, test_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")"
   ],
   "id": "891f2cb40509b580",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9461 - loss: 0.1534\n",
      "Test accuracy: 0.95\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T07:38:19.864257Z",
     "start_time": "2024-12-15T07:38:19.206215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for pic in TEST_PATH.iterdir():\n",
    "    print(f\"{pic.name}: {predict_image_class(pic)}\")"
   ],
   "id": "a7b592c918b500ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 44ms/step\n",
      "fs446_eggs_roi0.2922506200.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "fs446_eggs_roi1.3339894300.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "fs446_eggs_roi3.2768170400.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "fs446_eggs_roi3.3149332100.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step\n",
      "fs446_eggs_roi3.3289047500.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "fs534_copepods_roi1.4031192200.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "fs534_copepods_roi1.4048441100.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step\n",
      "fs534_copepods_roi1.4133164900.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "fs534_copepods_roi1.4183715200.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step\n",
      "training_diatoms_roi0.4070788301.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step\n",
      "training_diatoms_roi0.4073816201.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step\n",
      "training_diatoms_roi2.4413740400.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "training_diatoms_roi2.4620018000.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "training_diatoms_roi3.4488066800.tif.png: [0]\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\n",
      "training_diatoms_roi3.4503847800.tif.png: [0]\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
